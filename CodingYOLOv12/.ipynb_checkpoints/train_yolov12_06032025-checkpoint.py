# -*- coding: utf-8 -*-
"""train-yolov12-object-detection-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kkoDRVFlklF16Xz_34SDKdashiKvUJq5

[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)

# How to Train YOLOv12 Object Detection on a Custom Dataset

---

[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb)
[![arXiv](https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg)](https://arxiv.org/abs/2502.12524)
[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov12-model)
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/sunsmarterjie/yolov12)

[YOLOv12](https://github.com/sunsmarterjie/yolov12) is a newly proposed attention-centric variant of the YOLO family that focuses on incorporating efficient attention mechanisms into the backbone while preserving real-time performance. Instead of relying heavily on CNN-based architectures like its predecessors, YOLOv12 introduces a simple yet powerful ‚Äúarea attention‚Äù module, which strategically partitions the feature map to reduce the quadratic complexity of full self-attention. It also adopts residual efficient layer aggregation networks (R-ELAN) to enhance feature aggregation and training stability, especially for larger models. These innovations, together with refinements such as scaled residual connections and a reduced MLP ratio, enable YOLOv12 to harness the benefits of attention (e.g., better global context modeling) without sacrificing speed.

![yolov12-area-attention](https://media.roboflow.com/notebooks/examples/yolov12-area-attention.png)

Compared to prior YOLO iterations (e.g., YOLOv10, YOLOv11, and YOLOv8), YOLOv12 achieves higher detection accuracy with competitive or faster inference times across all model scales. Its five sizes‚ÄîN, S, M, L, and X‚Äîrange from 2.6M to 59.1M parameters, striking a strong accuracy‚Äìspeed balance. For instance, the smallest YOLOv12-N surpasses other ‚Äúnano‚Äù models by over 1% mAP with latency around 1.6 ms on a T4 GPU, and the largest YOLOv12-X achieves 55.2% mAP, comfortably outscoring comparable real-time detectors such as RT-DETR and YOLOv11-X . By matching or exceeding state-of-the-art accuracy while remaining fast, YOLOv12 represents a notable step forward for attention-based real-time object detection.

![yolov12-metrics](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/yolov12-metrics.png)

## Environment setup

### Configure your API keys

To fine-tune YOLOv12, you need to provide your Roboflow API key. Follow these steps:

- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.
- In Colab, go to the left pane and click on `Secrets` (üîë). Store Roboflow API Key under the name `ROBOFLOW_API_KEY`.

### Check GPU availability

**NOTE:** **YOLOv12 leverages FlashAttention to speed up attention-based computations, but this feature requires an Nvidia GPU built on the Ampere architecture or newer‚Äîfor example, GPUs like the RTX 3090, RTX 3080, or even the Nvidia L4 meet this requirement.**

Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`.
"""

!nvidia-smi

import os
HOME = os.getcwd()
print(HOME)

"""### Install dependencies

**NOTE:** Currently, YOLOv12 does not have its own PyPI package, so we install it directly from GitHub while also adding roboflow (to conveniently pull datasets from the Roboflow Universe), supervision (to visualize inference results and benchmark the model‚Äôs performance), and flash-attn (to accelerate attention-based computations via optimized CUDA kernels).

## K·∫øt n·ªëi t·ªõi google drive
"""

# Commented out IPython magic to ensure Python compatibility.
# 1. Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
3
# 2. Di chuy·ªÉn ƒë·∫øn th∆∞ m·ª•c b·∫°n mu·ªën l∆∞u trong Drive
# %cd /content/drive/MyDrive

!pip install -q git+https://github.com/sunsmarterjie/yolov12.git roboflow supervision flash-attn

"""### Download example data

Let's download an image we can use for YOLOv12 inference. Feel free to drag and drop your own images into the Files tab on the left-hand side of Google Colab, then reference their filenames in your code for a custom inference demo.
"""

!wget https://media.roboflow.com/notebooks/examples/dog.jpeg

"""## Run inference ( Test model )

In the example, we're using the `yolov12l.pt` model, but you can experiment with different model sizes by simply swapping out the model name during initialization. Options include `yolov12n.pt`, `yolov12s.pt`, `yolov12m.pt`, `yolov12l.pt`, and `yolov12x.pt`.

- T√¥i nghƒ© ƒë√¢y l√† source ƒë·ªÉ test ·∫£nh
"""

import cv2
from ultralytics import YOLO
import supervision as sv

image_path = f"{HOME}/dog.jpeg"
image = cv2.imread(image_path)

model = YOLO('yolov12l.pt')

results = model(image, verbose=False)[0]
detections = sv.Detections.from_ultralytics(results)

box_annotator = sv.BoxAnnotator()
label_annotator = sv.LabelAnnotator()

annotated_image = image.copy()
annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)
annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)

sv.plot_image(annotated_image)

"""## Download dataset from Roboflow Universe"""

from roboflow import Roboflow
import os
import shutil
# Kh·ªüi t·∫°o Roboflow v·ªõi API key
# rf = Roboflow(api_key="JutTY4tTGxWHjEqJiZNb")

# # Truy c·∫≠p workspace v√† project c·ªßa b·∫°n
# project = rf.workspace("nhan-dien-hanh-vi-trong-lop-hoc").project("student-behavior-recognition")

# # T·∫£i version 3 c·ªßa dataset v·ªõi format YOLOv5
# dataset = project.version(1).download("yolov12")

# # T·∫£i version 3 c·ªßa dataset v·ªõi format YOLOv5
# print(f"D·ªØ li·ªáu ƒë√£ t·∫£i v·ªÅ: {dataset.location}")

# ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ trong Google Drive
drive_dataset_path = '/content/drive/MyDrive/Colab_Notebooks/Dataset/ClassroomBehaviorDataset2'

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(drive_dataset_path, exist_ok=True)

# Sao ch√©p to√†n b·ªô th∆∞ m·ª•c dataset sang Google Drive
shutil.copytree(dataset.location, drive_dataset_path, dirs_exist_ok=True)

print(f"ƒê√£ sao ch√©p dataset ƒë·∫øn: {drive_dataset_path}")

# In ra c·∫•u tr√∫c th∆∞ m·ª•c ƒë·ªÉ ki·ªÉm tra
import subprocess
print("\nC·∫•u tr√∫c th∆∞ m·ª•c:")
subprocess.run(f"tree {drive_dataset_path}", shell=True)

# In ra c·∫•u tr√∫c th∆∞ m·ª•c ƒë·ªÉ ki·ªÉm tra
import subprocess
print("\nC·∫•u tr√∫c th∆∞ m·ª•c:")
subprocess.run(f"tree {drive_dataset_path}", shell=True)

!ls {dataset.location}

"""**NOTE:** We need to make a few changes to our downloaded dataset so it will work with YOLOv12. Run the following bash commands to prepare your dataset for training by updating the relative paths in the `data.yaml` file, ensuring it correctly points to the subdirectories for your dataset's `train`, `test`, and `valid` subsets."""

!sed -i '$d' {dataset.location}/data.yaml
!sed -i '$d' {dataset.location}/data.yaml
!sed -i '$d' {dataset.location}/data.yaml
!sed -i '$d' {dataset.location}/data.yaml
!echo -e "test: ../test/images\ntrain: ../train/images\nval: ../valid/images" >> {dataset.location}/data.yaml

!cat {dataset.location}/data.yaml

"""## Th·ªëng k√™ ·∫£nh
- Th·ªëng k√™ v·ªÅ ph√¢n b·ªï s·ªë l∆∞·ª£ng object file train, valid, test
- Th·ªëng k√™ s·ªë l∆∞·ª£ng theo l·ªõp
- Th·ªëng k√™ k√≠ch th∆∞·ªõc ·∫£nh

### Data yaml
"""

data_yaml_path = "/content/drive/MyDrive/Colab_Notebooks/Dataset/ClassroomBehaviorDataset/data.yaml"

import os
import yaml
from collections import defaultdict
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import cv2
import random
import seaborn as sns

def analyze_dataset(yaml_path):
    # 1. ƒê·ªçc v√† in th√¥ng tin YAML
    print("1. ƒê·ªçc file YAML:")
    with open(yaml_path, 'r') as f:
        data = yaml.safe_load(f)
    print(f"N·ªôi dung YAML: {data}")

    root_path = os.path.dirname(yaml_path)
    class_names = data['names']
    print(f"\nClasses: {class_names}")

    # 2. Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c
    print("\n2. C·∫•u tr√∫c th∆∞ m·ª•c:")
    print(f"Root path: {root_path}")
    print("N·ªôi dung root:")
    print(os.listdir(root_path))

    # 3. Thu th·∫≠p th·ªëng k√™
    stats = {
        'train': defaultdict(lambda: {'count': 0, 'sizes': [], 'examples': []}),
        'valid': defaultdict(lambda: {'count': 0, 'sizes': [], 'examples': []}),
        'test': defaultdict(lambda: {'count': 0, 'sizes': [], 'examples': []})
    }

    for split in ['train', 'valid', 'test']:
        images_dir = os.path.join(root_path, split, 'images')
        labels_dir = os.path.join(root_path, split, 'labels')

        print(f"\nX·ª≠ l√Ω {split} set:")
        print(f"Images dir: {images_dir}")
        print(f"Labels dir: {labels_dir}")

        if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
            print(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c images ho·∫∑c labels cho {split}")
            continue

        # In m·∫´u n·ªôi dung labels
        label_files = os.listdir(labels_dir)
        if label_files:
            print("\nM·∫´u n·ªôi dung labels:")
            for label_file in label_files[:2]:
                print(f"\nFile: {label_file}")
                with open(os.path.join(labels_dir, label_file), 'r') as f:
                    print(f.read())

        # X·ª≠ l√Ω t·ª´ng ·∫£nh
        for img_file in os.listdir(images_dir):
            if img_file.endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(images_dir, img_file)
                label_path = os.path.join(labels_dir, img_file.rsplit('.', 1)[0] + '.txt')

                if os.path.exists(label_path):
                    try:
                        img = cv2.imread(img_path)
                        if img is None:
                            print(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {img_path}")
                            continue
                        height, width = img.shape[:2]

                        with open(label_path, 'r') as f:
                            label_content = f.read().strip()
                            if label_content:
                                for line in label_content.split('\n'):
                                    try:
                                        parts = line.strip().split()
                                        if len(parts) == 5:
                                            class_id = int(parts[0])
                                            if class_id < len(class_names):
                                                class_name = class_names[class_id]
                                                stats[split][class_name]['count'] += 1
                                                stats[split][class_name]['sizes'].append((width, height))
                                                stats[split][class_name]['examples'].append((img_path, line))
                                        else:
                                            print(f"Format label kh√¥ng ƒë√∫ng: {line}")
                                    except ValueError as e:
                                        print(f"L·ªói parse label: {str(e)}")
                    except Exception as e:
                        print(f"L·ªói x·ª≠ l√Ω {img_path}: {str(e)}")

    # 4. Visualize k·∫øt qu·∫£
    # 4.1 Bi·ªÉu ƒë·ªì s·ªë l∆∞·ª£ng objects
    plt.figure(figsize=(12, 6))
    splits = list(stats.keys())
    x = np.arange(len(class_names))
    width = 0.25

    colors = ['#2ecc71', '#3498db', '#e74c3c']
    for i, (split, color) in enumerate(zip(splits, colors)):
        counts = [stats[split][class_name]['count'] for class_name in class_names]
        plt.bar(x + i*width, counts, width, label=split, color=color)

    plt.xlabel('Classes', fontsize=12)
    plt.ylabel('S·ªë l∆∞·ª£ng objects', fontsize=12)
    plt.title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng objects', fontsize=14)
    plt.xticks(x + width, class_names, rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # 4.2 Hi·ªÉn th·ªã ·∫£nh m·∫´u
    def plot_samples(split, class_name, num_samples=3):
        examples = stats[split][class_name]['examples']
        if not examples:
            return

        samples = random.sample(examples, min(num_samples, len(examples)))
        fig, axes = plt.subplots(1, len(samples), figsize=(15, 5))
        if len(samples) == 1:
            axes = [axes]

        for ax, (img_path, label) in zip(axes, samples):
            img = cv2.imread(img_path)
            if img is None:
                print(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {img_path}")
                continue

            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            height, width = img.shape[:2]

            try:
                parts = label.strip().split()
                if len(parts) == 5:
                    _, x_center, y_center, w, h = map(float, parts)

                    x1 = int((x_center - w/2) * width)
                    y1 = int((y_center - h/2) * height)
                    x2 = int((x_center + w/2) * width)
                    y2 = int((y_center + h/2) * height)

                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            except Exception as e:
                print(f"L·ªói v·∫Ω bbox: {str(e)}")

            ax.imshow(img)
            ax.axis('off')
            ax.set_title(f'{split} - {class_name}')

        plt.tight_layout()
        plt.show()

    print("\nHi·ªÉn th·ªã ·∫£nh m·∫´u:")
    for class_name in class_names:
        for split in splits:
            if stats[split][class_name]['count'] > 0:
                plot_samples(split, class_name)

    # 4.3 Bi·ªÉu ƒë·ªì k√≠ch th∆∞·ªõc ·∫£nh
    plt.figure(figsize=(10, 6))
    for split, color in zip(splits, colors):
        all_sizes = []
        for class_name in class_names:
            all_sizes.extend(stats[split][class_name]['sizes'])
        if all_sizes:
            widths, heights = zip(*all_sizes)
            plt.scatter(widths, heights, alpha=0.6, label=split, color=color)

    plt.xlabel('Width (pixels)')
    plt.ylabel('Height (pixels)')
    plt.title('Ph√¢n b·ªë k√≠ch th∆∞·ªõc ·∫£nh')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # 5. In th·ªëng k√™ chi ti·∫øt
    print("\nTh·ªëng k√™ chi ti·∫øt:")
    for split in stats:
        print(f"\n=== {split.upper()} ===")
        total = 0
        for class_name in stats[split]:
            count = stats[split][class_name]['count']
            sizes = stats[split][class_name]['sizes']
            total += count

            if count > 0:
                avg_width = np.mean([s[0] for s in sizes])
                avg_height = np.mean([s[1] for s in sizes])
                print(f"\nClass: {class_name}")
                print(f"- S·ªë l∆∞·ª£ng: {count}")
                print(f"- K√≠ch th∆∞·ªõc trung b√¨nh: {avg_width:.0f}x{avg_height:.0f}")

        print(f"\nT·ªïng s·ªë objects: {total}")
analyze_dataset(data_yaml_path)

"""## Fine-tune YOLOv12 model

We are now ready to fine-tune our YOLOv12 model. In the code below, we initialize the model using a starting checkpoint‚Äîhere, we use `yolov12s.yaml`, but you can replace it with any other model (e.g., `yolov12n.pt`, `yolov12m.pt`, `yolov12l.pt`, or `yolov12x.pt`) based on your preference. We set the training to run for 100 epochs in this example; however, you should adjust the number of epochs along with other hyperparameters such as batch size, image size, and augmentation settings (scale, mosaic, mixup, and copy-paste) based on your hardware capabilities and dataset size.

**Note:** **Note that after training, you might encounter a `TypeError: argument of type 'PosixPath' is not iterable error` ‚Äî this is a known issue, but your model weights will still be saved, so you can safely proceed to running inference.**

## K·∫øt n·ªëi google drive
"""

import os
import shutil
from datetime import datetime
from ultralytics import YOLO
from google.colab import drive

def get_unique_path(base_path):
    """
    T·∫°o ƒë∆∞·ªùng d·∫´n duy nh·∫•t b·∫±ng c√°ch th√™m phi√™n b·∫£n n·∫øu th∆∞ m·ª•c ƒë√£ t·ªìn t·∫°i

    Args:
        base_path (str): ƒê∆∞·ªùng d·∫´n g·ªëc mu·ªën t·∫°o

    Returns:
        str: ƒê∆∞·ªùng d·∫´n duy nh·∫•t
    """
    # N·∫øu th∆∞ m·ª•c ch∆∞a t·ªìn t·∫°i, tr·∫£ v·ªÅ ngay
    if not os.path.exists(base_path):
        return base_path

    # N·∫øu ƒë√£ t·ªìn t·∫°i, t√¨m phi√™n b·∫£n ti·∫øp theo
    version = 1
    while True:
        versioned_path = f"{base_path}_ver{version}"
        if not os.path.exists(versioned_path):
            return versioned_path
        version += 1

# Commented out IPython magic to ensure Python compatibility.
# T·∫°o ƒë∆∞·ªùng d·∫´n l∆∞u tr·ªØ v·ªõi th∆∞ m·ª•c theo ng√†y
current_date = datetime.now().strftime("%d%m%Y")
nameYoloFamily = 'yolov12'
### base
base_save_path = f'/content/drive/MyDrive/Colab_Notebooks/StaticModels/{nameYoloFamily}{current_date}/'

# L·∫•y ƒë∆∞·ªùng d·∫´n duy nh·∫•t ----------------------------------ƒê∆∞·ªùng d·∫´n n√†y c≈©ng l√† quan tr·ªçng. V√¨ c√°c b√°o c√°o d∆∞·ªõi ƒë·ªÅu c√†n ƒë·∫øn
unique_save_path = get_unique_path(base_save_path)
# T·∫°o th∆∞ m·ª•c
os.makedirs(unique_save_path, exist_ok=True)
# T·∫°o m√¥ h√¨nh v√† hu·∫•n luy·ªán
model_filename = f'{nameYoloFamily}{current_date}.pt'
full_model_path = os.path.join(unique_save_path, model_filename)
# T·∫°o m√¥ h√¨nh YOLO v√† c·∫•u h√¨nh
model = YOLO(f'{nameYoloFamily}.yaml')

# Thay ƒë·ªïi th∆∞ m·ª•c l√†m vi·ªác
# %cd "{unique_save_path}"

# Hu·∫•n luy·ªán m√¥ h√¨nh
path_data_yaml = '/content/drive/MyDrive/Colab_Notebooks/Dataset/ClassroomBehaviorDataset2/data.yaml'
#
results = model.train(
    data = path_data_yaml,
    epochs=5,
    patience=10,  # S·ªë epoch ƒë·ª£i ƒë·ªÉ d·ª´ng n·∫øu kh√¥ng c·∫£i thi·ªán
    save_period=1,  # L∆∞u checkpoint sau m·ªói epoch
    optimizer='Adam'  # L·ª±a ch·ªçn optimizer
)
# L∆∞u m√¥ h√¨nh cu·ªëi c√πng
model.save(full_model_path)


# In th√¥ng tin v·ªÅ m√¥ h√¨nh ƒë√£ l∆∞u
print(f"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {full_model_path}")

# Ki·ªÉm tra v√† in k√≠ch th∆∞·ªõc file
model_size = os.path.getsize(full_model_path) / (1024 * 1024)  # Chuy·ªÉn sang MB
print(f"K√≠ch th∆∞·ªõc m√¥ h√¨nh: {model_size:.2f} MB")

# ƒê·∫£m b·∫£o s·ª≠ d·ª•ng UTF-8 encoding
log_path = os.path.join(unique_save_path, 'training_log.txt')
with open(log_path, 'w', encoding='utf-8') as log_file:
    log_file.write(f"M√¥ h√¨nh: {nameYoloFamily}\n")
    log_file.write(f"Ng√†y hu·∫•n luy·ªán: {current_date}\n")
    log_file.write(f"S·ªë epochs: 5\n")
    log_file.write(f"K√≠ch th∆∞·ªõc m√¥ h√¨nh: {model_size:.2f} MB\n")
    log_file.write(f"ƒê∆∞·ªùng d·∫´n m√¥ h√¨nh: {full_model_path}\n")
    # log_file.write(f"ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c runs: {runs_destination_path}\n")

print(f"ƒê√£ ghi log t·∫°i: {log_path}")

print(f"ƒê√£ ghi log t·∫°i: {log_path}")

"""## Summary model

### Model info
"""

print(type(model.info()))
print(model.info())

model_summary = model.info()
if isinstance(model_summary, tuple):
    model_summary = model_summary[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n n·∫øu l√† tuple

!pip install torchinfo

"""## L∆∞u kh√°i qu√°t m√¥ h√¨nh"""

from torchinfo import summary
summary(model)



"""### Visualize ver 1 d·ª±a v√†o torchinfo"""

import matplotlib.pyplot as plt
import networkx as nx
import re

def parse_yolo_summary(summary_text):
    """Ph√¢n t√≠ch YOLO summary th√†nh danh s√°ch c√°c layers v√† k·∫øt n·ªëi."""
    G = nx.DiGraph()
    lines = summary_text.strip().split("\n")[2:-2]  # B·ªè d√≤ng ti√™u ƒë·ªÅ v√† t·ªïng k·∫øt

    prev_layer = None  # Theo d√µi l·ªõp tr∆∞·ªõc ƒë·ªÉ t·∫°o k·∫øt n·ªëi
    layer_colors = {
        'Conv': 'lightblue', 'C3k2': 'lightgreen', 'A2C2f': 'lightcoral',
        'Concat': 'orange', 'Upsample': 'purple', 'Detect': 'red'
    }

    for line in lines:
        match = re.search(r'(\w+): (\d+-\d+)\s+\(([\d,]+)?\)', line)
        if match:
            layer_type, layer_index, params = match.groups()
            params = int(params.replace(",", "")) if params else 0

            # Th√™m node
            G.add_node(layer_index, label=f"{layer_type}\n{layer_index}",
                       color=layer_colors.get(layer_type, 'gray'), size=params)

            # K·∫øt n·ªëi v·ªõi l·ªõp tr∆∞·ªõc ƒë√≥ (gi·∫£ ƒë·ªãnh n·ªëi tuy·∫øn t√≠nh)
            if prev_layer:
                G.add_edge(prev_layer, layer_index)

            prev_layer = layer_index  # C·∫≠p nh·∫≠t l·ªõp tr∆∞·ªõc ƒë√≥

    return G

def visualize_yolo(G):
    """V·∫Ω s∆° ƒë·ªì ki·∫øn tr√∫c YOLO."""
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, seed=42, k=1.2)

    # K√≠ch th∆∞·ªõc node d·ª±a tr√™n s·ªë l∆∞·ª£ng tham s·ªë
    node_sizes = [max(300, G.nodes[n]['size'] // 20) for n in G.nodes]
    node_colors = [G.nodes[n]['color'] for n in G.nodes]

    nx.draw(G, pos, with_labels=True, labels={n: G.nodes[n]['label'] for n in G.nodes},
            node_size=node_sizes, node_color=node_colors, font_size=10,
            font_weight='bold', edge_color='gray', arrows=True)

    plt.title("YOLOv12 Architecture", fontsize=14)
    plt.tight_layout()
    plt.show()

# D·ªØ li·ªáu ƒë·∫ßu v√†o (copy t·ª´ b·∫°n)
summary_text = """==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
YOLO                                                              --
‚îú‚îÄDetectionModel: 1-1                                             --
‚îÇ    ‚îî‚îÄSequential: 2-1                                            --
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-1                                             (464)
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-2                                             (4,672)
‚îÇ    ‚îÇ    ‚îî‚îÄC3k2: 3-3                                             (6,640)
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-4                                             (36,992)
‚îÇ    ‚îÇ    ‚îî‚îÄC3k2: 3-5                                             (26,080)
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-6                                             (147,712)
‚îÇ    ‚îÇ    ‚îî‚îÄA2C2f: 3-7                                            (180,864)
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-8                                             (295,424)
‚îÇ    ‚îÇ    ‚îî‚îÄA2C2f: 3-9                                            (689,408)
‚îÇ    ‚îÇ    ‚îî‚îÄUpsample: 3-10                                        --
‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-11                                          --
‚îÇ    ‚îÇ    ‚îî‚îÄA2C2f: 3-12                                           (86,912)
‚îÇ    ‚îÇ    ‚îî‚îÄUpsample: 3-13                                        --
‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-14                                          --
‚îÇ    ‚îÇ    ‚îî‚îÄA2C2f: 3-15                                           (24,000)
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-16                                            (36,992)
‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-17                                          --
‚îÇ    ‚îÇ    ‚îî‚îÄA2C2f: 3-18                                           (74,624)
‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-19                                            (147,712)
‚îÇ    ‚îÇ    ‚îî‚îÄConcat: 3-20                                          --
‚îÇ    ‚îÇ    ‚îî‚îÄC3k2: 3-21                                            (378,880)
‚îÇ    ‚îÇ    ‚îî‚îÄDetect: 3-22                                          (431,452)
==========================================================================================
"""

# X·ª≠ l√Ω v√† v·∫Ω s∆° ƒë·ªì
G = parse_yolo_summary(summary_text)
visualize_yolo(G)



"""## Evaluate fine-tuned YOLOv12 model

- M·ª•c ƒë√≠ch: ƒê·∫£m b·∫£o m√£ h√≥a k√Ω t·ª± UTF-8 ƒë∆∞·ª£c s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh
Gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ encoding khi l√†m vi·ªác v·ªõi c√°c k√Ω t·ª± qu·ªëc t·∫ø, ƒë·∫∑c bi·ªát l√† ti·∫øng Vi·ªát
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!ls {HOME}/runs/detect/train/

from IPython.display import Image

Image(filename=f'{unique_save_path}/runs/detect/train/confusion_matrix.png', width=1000)

"""- Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix) trong h√¨nh cho th·∫•y k·∫øt qu·∫£ ph√¢n lo·∫°i c·ªßa
m√¥ h√¨nh v·ªõi 2 l·ªõp: "student raising hand" v√† "background". H√£y ph√¢n t√≠ch t·ª´ng ph·∫ßn:
C√°c gi√° tr·ªã trong ma tr·∫≠n:

- 58: S·ªë l∆∞·ª£ng d·ª± ƒëo√°n ƒë√∫ng "student raising hand" (True Positive)

- 11: S·ªë l∆∞·ª£ng d·ª± ƒëo√°n sai "background" (False Negative)

- Kh√¥ng c√≥ tr∆∞·ªùng h·ª£p nh·∫ßm l·∫´n gi·ªØa c√°c l·ªõp (False Positive = 0)

**√ù nghƒ©a:**

- M√¥ h√¨nh kh√° t·ªët trong vi·ªác nh·∫≠n di·ªán h·ªçc sinh gi∆° tay (58 tr∆∞·ªùng h·ª£p ƒë√∫ng)

- C√≥ 11 tr∆∞·ªùng h·ª£p h·ªçc sinh gi∆° tay nh∆∞ng m√¥ h√¨nh kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c (b·ªè s√≥t)

- Kh√¥ng c√≥ tr∆∞·ªùng h·ª£p n√†o m√¥ h√¨nh nh·∫ßm l·∫´n background th√†nh h·ªçc sinh gi∆° tay

**3**. C√°c metrics c√≥ th·ªÉ t√≠nh:

- Precision = 58/(58+0) = 100% (ƒë·ªô ch√≠nh x√°c)

- Recall = 58/(58+11) ‚âà 84.1% (ƒë·ªô bao ph·ªß)

- F1-Score = 2 (Precision Recall)/(Precision + Recall) ‚âà 91.3%

Nh·∫≠n x√©t:
- M√¥ h√¨nh c√≥ ƒë·ªô ch√≠nh x√°c cao (kh√¥ng c√≥ false positive)
- C·∫ßn c·∫£i thi·ªán kh·∫£ nƒÉng ph√°t hi·ªán trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p b·ªã b·ªè s√≥t
Ph√π h·ª£p cho ·ª©ng d·ª•ng th·ª±c t·∫ø v√¨ √≠t g√¢y nh·∫ßm l·∫´n
"""

from IPython.display import Image

Image(filename=f'{unique_save_path}/runs/detect/train/results.png', width=1000)

import supervision as sv

ds = sv.DetectionDataset.from_yolo(
    images_directory_path=f"{unique_save_path}/test/images",
    annotations_directory_path=f"{unique_save_path}/test/labels",
    data_yaml_path=f"{}/data.yaml"
)

ds.classes

from supervision.metrics import MeanAveragePrecision

model = YOLO(f'/{HOME}/runs/detect/train/weights/best.pt')

predictions = []
targets = []

for _, image, target in ds:
    results = model(image, verbose=False)[0]
    detections = sv.Detections.from_ultralytics(results)

    predictions.append(detections)
    targets.append(target)

map = MeanAveragePrecision().update(predictions, targets).compute()

""" ![image.png](data:image/png;base64,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)"""

print("mAP 50:95", map.map50_95)
print("mAP 50", map.map50)
print("mAP 75", map.map75)
print(f'mAP: {map}')

import matplotlib.pyplot as plt
import numpy as np

# Example mAP results
map_50_95 = 0.8019801980198018
map_50 = 0.8019801980198018
map_75 = 0.8019801980198018
map_scores = [0.80198, 0.80198, 0.80198, 0.80198, 0.80198, 0.80198, 0.80198, 0.80198, 0.80198, 0.80198]
iou_thresh = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]

# Small objects
small_map_50_95 = 0.0
small_map_50 = 0.0
small_map_75 = 0.0
small_map_scores = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

# Medium objects
medium_map_50_95 = 0.0
medium_map_50 = 0.0
medium_map_75 = 0.0
medium_map_scores = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

# Large objects
large_map_50_95 = 0.9604
large_map_50 = 0.9604
large_map_75 = 0.9604
large_map_scores = [0.9604, 0.9604, 0.9604, 0.9604, 0.9604, 0.9604, 0.9604, 0.9604, 0.9604, 0.9604]

# Plotting mAP scores
plt.figure(figsize=(10, 6))

# Plot mAP scores for all objects
plt.plot(iou_thresh, map_scores, label='All objects', marker='o')

# Plot mAP scores for small objects
plt.plot(iou_thresh, small_map_scores, label='Small objects', marker='o')

# Plot mAP scores for medium objects
plt.plot(iou_thresh, medium_map_scores, label='Medium objects', marker='o')

# Plot mAP scores for large objects
plt.plot(iou_thresh, large_map_scores, label='Large objects', marker='o')

# Adding labels and title
plt.xlabel('IoU Threshold')
plt.ylabel('mAP Score')
plt.title('mAP Scores at Different IoU Thresholds')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

# Print detailed mAP results
print(f"mAP @ 50:95: {map_50_95:.4f}")
print(f"mAP @ 50: {map_50:.4f}")
print(f"mAP @ 75: {map_75:.4f}")
print(f"mAP scores: {map_scores}")
print(f"IoU thresholds: {iou_thresh}")

print("\nSmall objects:")
print(f"mAP @ 50:95: {small_map_50_95:.4f}")
print(f"mAP @ 50: {small_map_50:.4f}")
print(f"mAP @ 75: {small_map_75:.4f}")
print(f"mAP scores: {small_map_scores}")

print("\nMedium objects:")
print(f"mAP @ 50:95: {medium_map_50_95:.4f}")
print(f"mAP @ 50: {medium_map_50:.4f}")
print(f"mAP @ 75: {medium_map_75:.4f}")
print(f"mAP scores: {medium_map_scores}")

print("\nLarge objects:")
print(f"mAP @ 50:95: {large_map_50_95:.4f}")
print(f"mAP @ 50: {large_map_50:.4f}")
print(f"mAP @ 75: {large_map_75:.4f}")
print(f"mAP scores: {large_map_scores}")

"""# V·∫Ω map"""

map.plot()

"""## Run inference with fine-tuned YOLOv12 model"""

print(f'Duong dan: {unique_save_path}')

import supervision as sv

model = YOLO(f'/{unique_save_path}/runs/detect/train/weights/best.pt')
#--------------------- ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model. Nh·ªõ ƒë·ªÉ √Ω kƒ© xem ƒë√∫ng model ch∆∞a.
data_yaml_path = '/content/drive/MyDrive/Colab_Notebooks/Dataset/ClassroomBehaviorDataset2'

ds = sv.DetectionDataset.from_yolo(
    images_directory_path=f"{data_yaml_path}/test/images",
    annotations_directory_path=f"{data_yaml_path}/test/labels",
    data_yaml_path=f"{data_yaml_path}/data.yaml"
)

'''
√ù t∆∞·ªüng:
L·∫•y m·ªôt h√¨nh ·∫£nh ng·∫´u nhi√™n t·ª´ t·∫≠p d·ªØ li·ªáu.
D·ª± ƒëo√°n ƒë·ªëi t∆∞·ª£ng tr√™n ·∫£nh b·∫±ng m√¥ h√¨nh YOLO.
√Åp d·ª•ng NMS ƒë·ªÉ lo·∫°i b·ªè c√°c ph√°t hi·ªán tr√πng l·∫∑p.
V·∫Ω h·ªôp gi·ªõi h·∫°n v√† nh√£n l√™n ·∫£nh.
Hi·ªÉn th·ªã ·∫£nh ƒë√£ ƒë∆∞·ª£c ch√∫ th√≠ch.
'''
import random

i = random.randint(0, len(ds))

image_path, image, target = ds[i]

results = model(image, verbose=False)[0]
detections = sv.Detections.from_ultralytics(results).with_nms()

box_annotator = sv.BoxAnnotator()
label_annotator = sv.LabelAnnotator()

annotated_image = image.copy()
annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)
annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)

sv.plot_image(annotated_image)

import glob
from IPython.display import Image, display

for imageName in glob.glob(f'{unique_save_path}/runs/detect/train/*.jpg')[:10]: #assuming JPG
    display(Image(filename=imageName))